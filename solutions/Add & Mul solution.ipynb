{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = self.inbound_nodes[0].value + self.inbound_nodes[1].value\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients = {n: 0 for n in self.inbound_nodes}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad = n.gradients[self]\n",
    "            self.gradients[self.inbound_nodes[0]] += 1 * grad\n",
    "            self.gradients[self.inbound_nodes[1]] += 1 * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Forward Pass\n",
    "\n",
    "We take the value from both input nodes and add them together.\n",
    "\n",
    "## Backward Pass\n",
    "\n",
    "We first initialize or reset the gradients for each input node to its zero value. For a scalar this is simply 0, for an array it's an array of 0s. The derivative for $+$ is 1 for both inputs. We loop over all the output nodes and accumulate the gradients using the chain rule. Remember we consider all paths from the output to the input.\n",
    "\n",
    "The majority of the code for the backward pass will look the same for the rest of the solutions. The differences will be the number of input nodes and the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mul Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        self.cache[0] = self.inbound_nodes[0].value\n",
    "        self.cache[1] = self.inbound_nodes[1].value\n",
    "        val = self.cache[0] * self.cache[1]\n",
    "        self.value = val\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients = {n: 0 for n in self.inbound_nodes}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad = n.gradients[self]\n",
    "            self.gradients[self.inbound_nodes[0]] += self.cache[1] * grad\n",
    "            self.gradients[self.inbound_nodes[1]] += self.cache[0] * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "Similar to the Add node except we multiply the inputs this time. We also store the inputs values in the cache for use in the backward pass.\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "Recall the derivatives from the lab\n",
    "\n",
    "$$\n",
    "\\frac {\\partial Mul}{\\partial x} = y\n",
    "\\hspace{0.5in}\n",
    "\\frac {\\partial Mul}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "We use these in the backwards pass.\n",
    "\n",
    "```\n",
    "self.gradients[self.inbound_nodes[0]] += self.cache[1] * grad\n",
    "self.gradients[self.inbound_nodes[1]] += self.cache[0] * grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
