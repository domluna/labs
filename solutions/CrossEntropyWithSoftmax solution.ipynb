{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropyWithSoftmax Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: assume y is a vector with values 0-9\n",
    "# easier to work with than a one-hot encoding\n",
    "class CrossEntropyWithSoftmax(Node):\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x,y])\n",
    "        \n",
    "    # ... omit other functions\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        exp_x = np.exp(x)\n",
    "        probs = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def forward(self):\n",
    "        probs = self._softmax(self.input_nodes[0].value)\n",
    "        y = self.input_nodes[1].value\n",
    "        self.cache[0] = np.copy(probs)\n",
    "        self.cache[1] = np.copy(y)\n",
    "        n = probs.shape[0]\n",
    "        logprobs = -np.log(probs[range(n), y])\n",
    "        self.value = np.sum(logprobs) / n\n",
    "\n",
    "    # we know this is a loss so we can be a bit less generic here\n",
    "    # should have 0 output nodes\n",
    "    def backward(self):\n",
    "        assert len(self.output_nodes) == 0\n",
    "        self.dvalues = {n: np.zeros_like(n.value) for n in self.input_nodes}\n",
    "        # combined derivative of softmax and cross entropy\n",
    "        dprobs = self.cache[0]\n",
    "        y = self.cache[1]\n",
    "        n = dprobs.shape[0]\n",
    "        dprobs[range(n), y] -= 1\n",
    "        dprobs /= n\n",
    "        # leave the gradient for the 2nd node all 0s, we don't care about the gradient\n",
    "        # for the labels\n",
    "        self.dvalues[self.input_nodes[0]] = dprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "Let $z$ and $x$ be n-element vectors. $x$ is the input to the softmax function, $z$ the output.\n",
    "\n",
    "$$\n",
    "z_{i} = \\frac {e^{x_{i}}} {\\sum_{j=1}^n e^{x_{j}}}\n",
    "$$\n",
    "\n",
    "For cross entropy, following with the same $z$ and introducing $y$, where $y$ is a one hot encoding of a [categorical](https://en.wikipedia.org/wiki/Categorical_distribution) output ($[0, 0, ....., 1, 0, 0]$) the same length \n",
    "as $z$. The loss is\n",
    "\n",
    "$$\n",
    "loss = -\\sum_i y_i log(z_i)\n",
    "$$\n",
    "\n",
    "It's straightforward to see the result will be 0 except when for the index of $y$ where $y_i = 1$. In a sense it's redundant to have a one hot encoding and instead we reduce the n-element vector to a single element with the value of the index where the one hot encoding would be 1. This way we can simply index the output of the softmax.\n",
    "\n",
    "```\n",
    "logprobs = -np.log(probs[range(n), y])\n",
    "```\n",
    "\n",
    "\n",
    "## Backward Pass\n",
    "\n",
    "If we take a look at the softmax function we'll note we map from n inputs to n outputs and each input plays a role in computing the output. There are two general cases, we either differentiate with respect to $x_i$ (numerator) or any of the other inputs $x_j$, part of the denominator, where $j \\ne i$.\n",
    "\n",
    "We use the [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule) to find the derivative of the softmax function. $\\sum_{j=1}^n$ is abbreviated as $\\sum_j$.\n",
    "\n",
    "$$\n",
    "\\frac {\\partial z_{i} } {\\partial x_{i} } = \n",
    "\\frac { e^{x_{i}} \\sum_j e^{x_{j}} - e^{x_{i}} e^{x_{i}} } { (\\sum_j e^{x_{j}})^2 } = \n",
    "\\frac { e^{x_{i}} } { \\sum_j e^{x_{j}} } - \\frac { e^{x_{i}} } { \\sum_j e^{x_{j}} } \\frac { e^{x_{i}} } { \\sum_j e^{x_{j}} } = \n",
    "z_{i} - (z_{i})^2 = \n",
    "z_{i} (1 - z_{i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial z_{i} } {\\partial x_{j} } = \n",
    "-\\frac { e^{x_{i}} e^{x_{j}} } { (\\sum_j e^{x_{j}})^2 } = \n",
    "-\\frac { e^{x_{i}} } { \\sum_j e^{x_{j}} } \\frac { e^{x_{j}} } { \\sum_j e^{x_{j}} } = \n",
    "-z_{i} z_{j}\n",
    "$$\n",
    "\n",
    "Then it follows from following the outputs back to the an input $x_k$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial z}{\\partial x_k} =\n",
    "\\sum_{p=1}^n \\frac {\\partial z_p} {\\partial x_k}\n",
    "$$\n",
    "\n",
    "where in out those $n$ additions, 1 of them is where $x_k$ is the numerator and the other $n-1$ are when $x_k$ is part of the denominator. In total we end up with an $n$ by $n$ matrix of derivatives known as the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant).\n",
    "\n",
    "Remember how all the values in the Cross Entropy go to 0 except where we picked based on the target label? Well, as it turns out that means the gradient of the output edge to every index of $z$ that isn't picked is now 0.\n",
    "\n",
    "$$\n",
    "\\frac {\\partial z}{\\partial x_k} =\n",
    "\\sum_{p=1}^n \\frac {\\partial z_p} {\\partial x_k}\n",
    "$$\n",
    "\n",
    "Is now either\n",
    "\n",
    "$$\n",
    "\\frac {\\partial z}{\\partial x_k} =\n",
    "\\frac {\\partial z_k} {\\partial x_k} = \n",
    "z_k (1 - z_k)\n",
    "\\hspace{0.25in}\n",
    "or\n",
    "\\hspace{0.25in}\n",
    "\\frac {\\partial z}{\\partial x_k} =\n",
    "\\frac {\\partial z_p} {\\partial x_k} =\n",
    "z_P z_k\n",
    "$$\n",
    "\n",
    "The derivative for the loss the easiest bit\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss} {\\partial z_i} = \\frac {-1} {z_i}\n",
    "$$\n",
    "\n",
    "Putting it all together\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss} {\\partial x_i} = \n",
    "\\frac {\\partial z_i} {\\partial x_i} \\frac {\\partial loss} {\\partial z_i} = \n",
    "z_i (1 - z_i) \\frac {-1} {z_i} = \n",
    "z_i - 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss} {\\partial x_j} = \n",
    "\\frac {\\partial z_i} {\\partial x_j} \\frac {\\partial loss} {\\partial z_i} = \n",
    "-z_i z_j \\frac {-1} {z_i} = \n",
    "z_j\n",
    "$$\n",
    "\n",
    "Here it is in code\n",
    "\n",
    "```\n",
    "dprobs[range(n), y] -= 1\n",
    "dprobs /= n\n",
    "```\n",
    "\n",
    "`dprobs` is a cache of the computed probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
